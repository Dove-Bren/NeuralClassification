\documentclass{article}
\begin{document}



\author{Jonathan Brandt, Skyler Manzanares, William Fong}
\date{April 2, 2015}
\title{Programming Assignment 2}

\maketitle

%Feel free to remove the 'description' text i placed (e.g. "Our hopfield network was terrific! Type stuff here")
\section{Overview}

\section{Hopfield Autoassociative Memory Network}
Our hopfield network was terrific! Type stuff here

\subsection{Image Encoding}
The image encoding... blah blah. Type here

\subsection{Performance}
Here is where we need to talk about how long it took and how good it was. We
also might want to mention the size limitations this introduced!

\subsection{Sensitivity}
This section is about how badly noise and the size disrupted our accuracy

\subsection{Comments}
It says "your own observation and conclusions about the experiment."
I'm thinking we talk generally about how it did (like a conclusion) and how
what we did ended up working.





\section{Hamming-Max Network}
The Hamming-Max was fairly straight forward to construct.  We first created the hammingnet portion which was given the input vector and weight matrix.  It computed the hamming distance of the input from each column vector prototype of the weight matrix, storing those values in an output vector, equal in width to the weight matrix.  We then constructed the Maxnet which was given the Hammingnet output as a parameter.  It subtracts an epsilon (which we specified as the smallest element in the hamming vector) from each element until a single zero value is found, declaring it the smallest hamming distance.  This index represents the prototype which is most similar to the input, suggesting the input must be of this classification.

\subsection{Image Encoding}
Deciding how to encode our images was a bit tricky.  We played with a few different ways of obtaining image outlines and decided for this network simply finding images already in outline form would be best.  We removed inner detail from these outlines and ran them through a program we wrote.  First, the images are resized so that they are all one standard size,  the images are then grayscaled and run through a threshhold function so that a clear outline of black on white is present.  To establish the prototypes for our weight matrix, we added the values of each representative image matrix to eachother, then averaged the values and processed the result as before.  We adjusted our threshhold function to maintain clarity.  We found that visually, too small a size diminshed image definition, while too large was time and resource intensive and lacked definitive shape for the prototype matrix.

\subsection{Performance}
Here is where we need to talk about how long it took and how good it was. We
also might want to mention the size limitations this introduced!

\subsection{Sensitivity}
This section is about how badly noise and the size disrupted our accuracy

\subsection{Comments}
It says "your own observation and conclusions about the experiment."
I'm thinking we talk generally about how it did (like a conclusion) and how
what we did ended up working.




\section{Bidirection Associative Memory (BAM)}

\subsection{Image Encoding}
All images being imported into BAM are required to be in a JPG format. These
images are then imported, resized into a standard format, and then polarized.
Each polarized image is then translated into a single dimensional vector.
Each 'class' of images are grouped together column wise to create a centroid
matrix.

\subsection{Performance}
The Bidirectional Associative Memory implementation performed rather poorly.
Most of the associations were directed towards a specific centroid, resulting
in at least 1 'class' performing extremely well. The results of our
experiment are as follows.
Animal Results:
    Dog:
        7 Correctly Classified
        3 Incorrectly Classified
    Elephant:
        1 Correctly Classified
        9 Incorrectly Classified
    Giraffe:
        0 Correctly Classified
        9 Correctly Classifed
Plane Results:
    Biplane:
        0 Correctly Classified
        10 Incorrectly Classified
    Jet Results:
        5 Correctly Classified
        5 Incorrectly Classified
    Airbus Results:
        0 Correctly Classified
        7 Incorrectly Classified
Marine Results:
    Ships:
        0 Correctly Classified
        5 Incorrectly Classified
    Submarine Results:
        8 Correctly Classified
        0 Incorrectly Classified
    Whale Results:
        3 Correctly Classified
        7 Incorrectly Classified

It seems that the BAM implementation attempted to classify nearly all animals
as dogs. For planes the network had more diverse attempts to classify each
object, although incorrectly. The marine results showed the most diverse and
correct classifications. Although it does seem that BAM attempted to describe
everything as either a submarine or a whale.

\subsection{Sensitivity}
This section is about how badly noise and the size disrupted our accuracy

\subsection{Comments}
It says "your own observation and conclusions about the experiment."
I'm thinking we talk generally about how it did (like a conclusion) and how
what we did ended up working.




\section{LVQ}
Our LVQ network was created using the Neural Net toolkit for Matlab. We selected a hidden-layer size of [2 * n] to allow the target classes to be created with little room for error in the way of dying centroids based on the order of training. Making use of the built-in training of the toolkit, we simply constructed an input matrix consisting of all of our outlines for each group and made target vectors to use with them. 
\subsection{Image Encoding}
Images were encoded by simply importaing the RGB map, grayscaling, and converting the grayscale 0-255 to -1 and 1 based on a threshold (taken to be 127). These images were also resized to the global size of 64 x 48. The resultant images were those that were thrown into the input matrix and used to train the lvq network.

\subsection{Performance}
Here is where we need to talk about how long it took and how good it was. We
also might want to mention the size limitations this introduced!

\subsection{Sensitivity}
This section is about how badly noise and the size disrupted our accuracy

\subsection{Comments}
It says "your own observation and conclusions about the experiment."
I'm thinking we talk generally about how it did (like a conclusion) and how
what we did ended up working.

 
\section{Member Assignments}
All members were involved in some manner in all major area of the project. These
major areas include the paper writup, the presentation creation, and the 
programming. The specific assignements per group member are given below, but no
single member handled an entire area of development.
\subsection{Jonathan Brandt}
\subsubsection{Programming}
Jonathan's programming work was divided between implementation of the 
Hamming/Max network and the creation of centroids for the networks based on a
large number of sample inputs. He was completely responsible for creating the Hamming layer of the Hamming/MAX network, and worked in conjunction with the rest of the team when creating the centroids. 

\subsubsection{Project Report}
Jonathan was tasked with the Hamming/Max section. He also was in charge of reporting on the Overview section.

\subsubsection{Project Presentation}
NOTHING HE DID NOTHING 0 percent FOR HIM....DAMN YOU SKYLER

\subsection{Skyler Manzanares}

\subsubsection{Programming}
Skyler implemented the formatting of centroid images to the weight matrix to be fed into the Hamming/MAX network. He also did extensive work creating the functions that read in and format images. Skyler was also responsible for implementing
the HAM network and the LVQ network.

\subsubsection{Project Report}
Skyler created the template for the project report and was in charge of the HAM 
network section and the Member Assignment section.

\subsubsection{Project Presentation}
lulz he ain't done a thing.  0\% for you

\subsection{William Fong}
\subsubsection{Programming}
William worked to create the Max layer of the Hamming/Max network and coordinate 
inputs and outputs of the various layers, summing all components together to create a working Hamming/Max network. William was also in charge of implementing the BAM network. He also was in charge of testing his BAM network creation.
\subsubsection{Project Report}
William was tasked to the BAM section and the LVQ analysis section.

\subsubsection{Project Presentation}
nuthin. no soup for you 


\end{document}
